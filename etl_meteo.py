import pandas as pd
import json
from datetime import datetime
import os 
from pymongo import MongoClient
from pymongo.errors import ConnectionFailure, OperationFailure

# --- 0. CONFIGURATION MONGO DB ET FICHIERS ---

# URI de votre instance MongoDB Compass locale
MONGO_URI = "mongodb://localhost:27017/" 
MONGO_DATABASE = "meteo_projet"            # Base de donnÃ©es cible
MONGO_COLLECTION = "donnees_horaires"      # Collection cible

# --- DÃ‰FINITIONS DES CHEMINS DE FICHIERS LOCAUX (DOIVENT CORRESPONDRE AUX NOMS DE VOS FICHIERS) ---
csv_files_la_madeleine = {
    '2024-10-07': "Weather+Underground+-+La+Madeleine,+FR.xlsx - 071024.csv",
    '2024-10-06': "Weather+Underground+-+La+Madeleine,+FR.xlsx - 061024.csv",
    '2024-10-05': "Weather+Underground+-+La+Madeleine,+FR.xlsx - 051024.csv",
    '2024-10-04': "Weather+Underground+-+La+Madeleine,+FR.xlsx - 041024.csv",
    '2024-10-03': "Weather+Underground+-+La+Madeleine,+FR.xlsx - 031024.csv",
    '2024-10-02': "Weather+Underground+-+La+Madeleine,+FR.xlsx - 021024.csv",
    '2024-10-01': "Weather+Underground+-+La+Madeleine,+FR.xlsx - 011024.csv"
}
csv_files_ichtegem = {
    '2024-10-07': "Weather+Underground+-+Ichtegem,+BE.xlsx - 071024.csv",
    '2024-10-06': "Weather+Underground+-+Ichtegem,+BE.xlsx - 061024.csv",
    '2024-10-05': "Weather+Underground+-+Ichtegem,+BE.xlsx - 051024.csv",
    '2024-10-04': "Weather+Underground+-+Ichtegem,+BE.xlsx - 041024.csv",
    '2024-10-03': "Weather+Underground+-+Ichtegem,+BE.xlsx - 031024.csv",
    '2024-10-02': "Weather+Underground+-+Ichtegem,+BE.xlsx - 021024.csv",
    '2024-10-01': "Weather+Underground+-+Ichtegem,+BE.xlsx - 011024.csv"
}
JSON_FILE_PATH = "Data_Source1_011024-071024.json"

# DÃ©finition des champs numÃ©riques Ã  vÃ©rifier dans MongoDB
NUMERIC_FIELDS = ['temperature_c', 'humidite_pct', 'pression_hpa', 'vent_vitesse_ms', 'pluie_accum_mm']

# --- 1. FONCTIONS DE TRANSFORMATION ET DE VÃ‰RIFICATION ---

def clean_value(value):
    """Nettoie une valeur potentiellement string/NaN pour la convertir en float."""
    if pd.isna(value): return None
    value = str(value).replace(',', '.').replace(' ', '').replace('Â°F', '').replace('mph', '').replace('in', '').replace('w/mÂ²', '').replace('%', '')
    try: return float(value)
    except ValueError: return None

def clean_and_convert_csv_df(df, station_id, source):
    """Nettoie le DataFrame CSV et effectue les conversions d'unitÃ©s (ImpÃ©rial -> MÃ©trique), retourne le DataFrame."""
    df_clean = pd.DataFrame()
    
    # Conversions (Noms de colonnes confirmÃ©s : Temperature, Humidity, Speed, Precip. Accum., Pressure, Time)
    df_clean['temperature_c'] = df['Temperature'].apply(clean_value).apply(lambda f: (f - 32) * 5/9 if f is not None else None)
    df_clean['humidite_pct'] = df['Humidity'].apply(clean_value)
    df_clean['pression_hpa'] = df['Pressure'].apply(clean_value).apply(lambda i: i * 33.8638 if i is not None else None) # Conversion inHg -> hPa
    df_clean['vent_vitesse_ms'] = df['Speed'].apply(clean_value).apply(lambda m: m * 0.44704 if m is not None else None) # Conversion mph -> m/s
    df_clean['pluie_accum_mm'] = df['Precip. Accum.'].apply(clean_value).apply(lambda i: i * 25.4 if i is not None else None) # Conversion in -> mm

    df_clean['date_heure_utc'] = df['Time'].apply(lambda t: t if isinstance(t, str) else None).apply(lambda t: f"{df.name} {t}" if t else None)

    df_clean['id_station'] = station_id
    df_clean['source_donnees'] = source
    
    df_clean = df_clean.dropna(subset=['date_heure_utc'])
    
    final_cols = ['date_heure_utc', 'temperature_c', 'humidite_pct', 'pression_hpa', 'vent_vitesse_ms', 'id_station', 'source_donnees', 'pluie_accum_mm']
    return df_clean[final_cols]

def check_initial_integrity(df_clean, station_id, date_str):
    """VÃ©rifie l'intÃ©gritÃ© des donnÃ©es CSV aprÃ¨s le nettoyage."""
    doublons = df_clean.duplicated(subset=['date_heure_utc']).sum()
    manquants_critiques = df_clean['temperature_c'].isnull().sum()
    
    if doublons > 0 or manquants_critiques > 0:
        print(f"   -> âŒ INTÃ‰GRITÃ‰ ({station_id} - {date_str}) : Doublons: {doublons}, TempÃ©rature manquante: {manquants_critiques}")
    else:
        print(f"   -> âœ… INTÃ‰GRITÃ‰ ({station_id} - {date_str}) : PassÃ©e.")

def clean_and_convert_json(json_hourly_data):
    """Nettoie et convertit les donnÃ©es JSON (Infoclimat)."""
    all_json_records = []
    
    for station_id, records in json_hourly_data.items():
        if not isinstance(records, list):
            print(f"   -> âš ï¸ AVERTISSEMENT JSON: DonnÃ©es inattendues pour la station {station_id}. IgnorÃ©.")
            continue
            
        for record in records:
            if not isinstance(record, dict):
                continue

            vent_moyen_ms = float(record.get('vent_moyen', 0) or 0) / 3.6
            pluie_mm = float(record.get('pluie_1h', record.get('pluie_3h', 0) or 0) or 0)
            
            # Format compatible MongoDB (dictionnaire)
            new_record = {
                'date_heure_utc': record.get('dh_utc'),
                'temperature_c': float(record.get('temperature')) if record.get('temperature') else None,
                'humidite_pct': int(record.get('humidite')) if record.get('humidite') else None,
                'pression_hpa': float(record.get('pression')) if record.get('pression') else None,
                'vent_vitesse_ms': vent_moyen_ms,
                'id_station': record.get('id_station'),
                'source_donnees': 'Infoclimat', 
                'pluie_accum_mm': pluie_mm
            }
            all_json_records.append(new_record)
            
    return all_json_records

def check_final_integrity(data_list):
    """VÃ©rifie l'intÃ©gritÃ© de la liste complÃ¨te de documents unifiÃ©s avant l'insertion MongoDB."""
    if not data_list: return False
        
    df_final = pd.DataFrame(data_list)
    print("\n--- âœ… Rapport d'IntÃ©gritÃ© UnifiÃ©e (Source Totale) ---")
    
    # Doublons globaux (mÃªme horodatage ET mÃªme station)
    duplicates = df_final.duplicated(subset=['date_heure_utc', 'id_station']).sum()
    print(f"-> Total des enregistrements : {len(df_final)}")
    print(f"-> Doublons trouvÃ©s (date/station) : {duplicates}")

    # RÃ©sumÃ© des valeurs manquantes
    print("\n-> RÃ©sumÃ© des valeurs manquantes par colonne :")
    print(df_final.isnull().sum())
    
    # Plage de DonnÃ©es
    df_final['date_heure_utc'] = pd.to_datetime(df_final['date_heure_utc'], errors='coerce')
    start_date = df_final['date_heure_utc'].min()
    end_date = df_final['date_heure_utc'].max()
    print(f"\n-> PÃ©riode couverte : Du {start_date} au {end_date}")

    if duplicates > 0:
        print("âŒ AVERTISSEMENT: Des doublons ont Ã©tÃ© trouvÃ©s. MongoDB les insÃ©rera, sauf si vous ajoutez un index unique.")
    print("-----------------------------------------------------")
    return True


# --- 2. FONCTIONS D'EXTRACTION ---

def read_full_json_file(file_path):
    """Lit et charge le contenu JSON complet depuis un fichier local."""
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"âŒ ERREUR CRITIQUE: Fichier JSON non trouvÃ©: {file_path}. VÃ©rifiez le nom.")
        
    with open(file_path, 'r', encoding='utf-8') as f:
        full_content = json.load(f)
    return full_content.get('hourly', {})

def extract_and_process_csv_from_disk(file_paths_dict, station_id, source):
    """Lit les fichiers CSV, les nettoie, les convertit et vÃ©rifie leur intÃ©gritÃ©."""
    all_records = []
    
    for date_str, file_name in file_paths_dict.items():
        try:
            # Correction: Encodage 'latin-1' ET dÃ©limiteur 'sep=;'
            df_csv = pd.read_csv(file_name, header=0, skiprows=[2], encoding='latin-1', sep=';') 
            
            # Nettoyage des noms de colonnes pour Ã©viter les espaces
            df_csv.columns = [col.strip() for col in df_csv.columns]
            
            df_csv.name = date_str 
            df_csv.insert(0, 'Date', date_str) 
            
            # Transformation (T)
            df_clean = clean_and_convert_csv_df(df_csv, station_id=station_id, source=source)
            
            # VÃ©rification initiale
            check_initial_integrity(df_clean, station_id, date_str)
            
            records = df_clean.to_dict('records')
            all_records.extend(records)
            print(f"   -> TraitÃ© {source} ({date_str} - {file_name}): {len(records)} lignes.")
        except FileNotFoundError:
            print(f"   -> âŒ ERREUR: Fichier CSV non trouvÃ©: {file_name}.")
        except Exception as e:
            print(f"   -> âŒ ERREUR lors du traitement de {file_name}: {e}")
            
    return all_records

# --- 3. FONCTIONS DE CHARGEMENT VERS MONGODB ---

def load_data_to_mongodb(data_list):
    """Connecte Ã  MongoDB, insÃ¨re la liste complÃ¨te des documents, et vÃ©rifie la collection cible."""
        
    try:
        # 1. Connexion
        client = MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)
        client.admin.command('ping') 
        print(f"âœ… Connexion MongoDB rÃ©ussie Ã  l'URI : {MONGO_URI}")
        
        # 2. Insertion
        db = client[MONGO_DATABASE]
        collection = db[MONGO_COLLECTION]
        
        collection.delete_many({}) # Purgement de la collection avant l'insertion
        print(f"   -> Collection '{MONGO_COLLECTION}' purgÃ©e. Insertion de {len(data_list)} documents...")
        
        result = collection.insert_many(data_list)
        
        # 3. VÃ‰RIFICATION FINALE DANS MONGODB (Compte)
        target_count = collection.count_documents({})

        print("\n--- âœ… Rapport d'IntÃ©gritÃ© Cible (MongoDB - Compte) ---")
        if target_count == len(data_list):
             print(f"-> âœ… SUCCÃˆS: Nombre d'enregistrements (Source: {len(data_list)}, Cible: {target_count}) correspond.")
        else:
             print(f"-> âŒ Ã‰CHEC: La source ({len(data_list)}) ne correspond pas Ã  la cible ({target_count}).")

        print("-------------------------------------------------------")
        client.close()
        return True
    
    except ConnectionFailure:
        print("\nâŒ ERREUR DE CONNEXION: Assurez-vous que votre serveur MongoDB local est dÃ©marrÃ© (via Compass) et que l'URI est correct.")
        return False
    except Exception as e:
        print(f"\nâŒ ERREUR LORS DU CHARGEMENT MONGODB : {e}")
        return False


# --- 4. NOUVELLE FONCTION D'AUDIT POST-MIGRATION ---

def audit_mongodb_data():
    """VÃ©rifie les types et les valeurs nulles des donnÃ©es aprÃ¨s la migration dans MongoDB."""
    
    print("\n--- ğŸ”¬ AUDIT POST-MIGRATION MONGO DB (QualitÃ© des DonnÃ©es) ---")
    
    try:
        client = MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)
        db = client[MONGO_DATABASE]
        collection = db[MONGO_COLLECTION]
        
        audit_success = True
        
        # 1. VÃ©rification des types et des nulles (MongoDB Aggregation)
        pipeline = []
        for field in NUMERIC_FIELDS:
            # Compte le nombre de documents oÃ¹ le champ est manquant (null)
            pipeline.append({
                "$group": {
                    "_id": None,
                    f"nulls_{field}": {"$sum": {"$cond": [{"$eq": [f"${field}", None]}, 1, 0]}}
                }
            })
            
        # Simplification de l'agrÃ©gation en un seul appel
        audit_results = list(collection.aggregate(pipeline))

        if audit_results:
            results = audit_results[0]
            del results['_id']
            
            print("\n-> VÃ©rification des valeurs manquantes (Nulles):")
            for key, count in results.items():
                field_name = key.replace('nulls_', '')
                if count > 0:
                    print(f"   âŒ {field_name.upper()} : {count} documents contiennent la valeur null.")
                    audit_success = False
                else:
                    print(f"   âœ… {field_name.upper()} : 0 valeur nulle (PassÃ©e).")

        # 2. VÃ©rification des plages de valeurs (optionnel mais bon)
        # On peut exÃ©cuter un simple find pour s'assurer qu'aucune chaÃ®ne n'a Ã©tÃ© insÃ©rÃ©e
        
        # Assurez-vous qu'aucun type String n'est prÃ©sent dans les champs numÃ©riques
        print("\n-> VÃ©rification des types (chaÃ®nes de caractÃ¨res dans champs numÃ©riques):")
        for field in NUMERIC_FIELDS:
            # Cherche si le champ existe et est de type String (BSON type 2)
            string_count = collection.count_documents({field: {"$type": "string"}})
            if string_count > 0:
                print(f"   âŒ {field.upper()} : {string_count} documents contiennent une chaÃ®ne de caractÃ¨res au lieu d'un nombre.")
                audit_success = False
            else:
                print(f"   âœ… {field.upper()} : Type numÃ©rique correct (PassÃ©e).")


        if audit_success:
            print("\nğŸ‰ AUDIT RÃ‰USSI: La qualitÃ© des donnÃ©es est maintenue aprÃ¨s la migration.")
        else:
             print("\nâš ï¸ AVERTISSEMENT: L'audit a dÃ©tectÃ© des problÃ¨mes de qualitÃ© (nulles/types).")

        print("==========================================================================")
        client.close()

    except ConnectionFailure:
        print("\nâŒ ERREUR DE CONNEXION: Impossible d'auditer. Le serveur MongoDB est-il toujours dÃ©marrÃ© ?")
    except Exception as e:
        print(f"\nâŒ ERREUR LORS DE L'AUDIT MONGODB : {e}")

# --- 5. ORCHESTRATION DU PIPELINE E-T-L FINAL ---

def run_full_etl():
    """ExÃ©cute l'intÃ©gralitÃ© du pipeline E-T-L pour MongoDB, suivi de l'audit."""
    
    all_processed_records = []
    
    print("--- â³ PHASE 1: Traitement des 14 fichiers CSV complets (Weather Underground) ---")
    madeleine_records = extract_and_process_csv_from_disk(csv_files_la_madeleine, station_id="1001", source="Weather Underground")
    all_processed_records.extend(madeleine_records)

    ichtegem_records = extract_and_process_csv_from_disk(csv_files_ichtegem, station_id="1002", source="Weather Underground")
    all_processed_records.extend(ichtegem_records)

    print("\n--- â³ PHASE 2: Traitement du fichier JSON complet (Infoclimat) ---")
    
    try:
        json_raw_content = read_full_json_file(JSON_FILE_PATH)
        print(f"   -> JSON Infoclimat complet chargÃ©.")
        
        json_records = clean_and_convert_json(json_raw_content)
        all_processed_records.extend(json_records)
        print(f"   -> TraitÃ© Infoclimat: {len(json_records)} lignes.")
    except FileNotFoundError:
        pass 
    except Exception as e:
        print(f"   -> âŒ ERREUR critique lors du traitement du fichier JSON: {e}")
    
    final_count = len(all_processed_records)
    print(f"\n--- âœ… PHASE 3: DonnÃ©es unifiÃ©es : {final_count} enregistrements totaux ---")
    
    if final_count > 0:
        # VÃ©rification finale avant chargement (Source)
        check_final_integrity(all_processed_records) 
        
        print("--- ğŸš€ PHASE 4: Chargement des donnÃ©es complÃ¨tes vers MongoDB ---")
        load_success = load_data_to_mongodb(all_processed_records)
        
        # NOUVELLE Ã‰TAPE : AUDIT POST-MIGRATION
        if load_success:
            audit_mongodb_data()
    else:
        print("âš ï¸ Le pipeline s'arrÃªte car aucune donnÃ©e n'a pu Ãªtre traitÃ©e.")


if __name__ == "__main__":
    run_full_etl()